#!/usr/bin/env python3
"""
Multi-session matrix benchmarking with persistent results.

This script runs strategy × model benchmarks and persists results
for cross-session comparison. Run multiple times to demonstrate
how episodic learning improves performance across sessions.

Usage:
  # Session 1 - baseline (cold start)
  uv run python scripts/run_matrix_benchmark.py --session 1 --storage .benchmark_memory

  # Session 2 - with learnings from session 1
  uv run python scripts/run_matrix_benchmark.py --session 2 --storage .benchmark_memory

  # Compare sessions
  uv run python scripts/run_matrix_benchmark.py --compare --output results

Results are saved to:
  - results/session_N_summary.md (human-readable)
  - results/session_N_raw.json (machine-readable)
  - results/comparison.md (after 2+ sessions)
"""

import argparse
import json
import os
import sys
from datetime import datetime
from pathlib import Path

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

# Load environment variables
from dotenv import load_dotenv
load_dotenv()

from src.core.types import Task, BenchmarkConfig
from src.executor.llm import LLMExecutor
from src.evaluator.exact_match import ExactMatchEvaluator
from src.llm.claude import ClaudeClient
from src.strategies.none import NoImprovementStrategy
from src.strategies.reflection import ReflectionStrategy
from src.strategies.episodic_memory import EpisodicMemoryStrategy
from src.storage.memory import InMemoryStorage
from src.storage.file import FileStorage
from src.benchmark.matrix_runner import MatrixBenchmarkRunner
from src.benchmark.model_config import ModelConfig
from src.benchmark.tasks import create_task_suite


def create_output_dir(output_path: Path) -> Path:
    """Create output directory if it doesn't exist."""
    output_path.mkdir(parents=True, exist_ok=True)
    return output_path


def get_timestamp_prefix() -> str:
    """Get a timestamp prefix for file naming."""
    return datetime.now().strftime("%Y%m%d_%H%M%S")


def save_results_json(results: dict, output_path: Path, session: int, timestamp: str) -> Path:
    """Save raw results to JSON."""
    filepath = output_path / f"{timestamp}_session_{session}_raw.json"
    with open(filepath, "w") as f:
        json.dump(results, f, indent=2, default=str)
    return filepath


def save_results_markdown(results: dict, output_path: Path, session: int, episode_count: int, timestamp: str) -> Path:
    """Save human-readable summary to markdown."""
    filepath = output_path / f"{timestamp}_session_{session}_summary.md"

    lines = [
        f"# Benchmark Results - Session {session}",
        "",
        f"**Date:** {results['timestamp']}",
        f"**Episodes at start:** {episode_count}",
        f"**Tasks:** {results['num_tasks']}",
        f"**Models:** {', '.join(results['models'])}",
        f"**Strategies:** {', '.join(results['strategies'])}",
        "",
        "## Performance Matrix",
        "",
        "| Strategy | Model | Pass Rate | Avg Attempts | Avg Score |",
        "|----------|-------|-----------|--------------|-----------|",
    ]

    for row in results["matrix"]:
        lines.append(
            f"| {row['strategy']} | {row['model']} | {row['pass_rate']:.1%} | "
            f"{row['avg_attempts']:.2f} | {row['avg_score']:.2f} |"
        )

    lines.extend([
        "",
        "## Best Performers",
        "",
    ])

    for i, best in enumerate(results.get("best_performers", [])[:3], 1):
        lines.append(f"{i}. **{best['strategy']} + {best['model']}**: {best['pass_rate']:.1%} pass rate")

    lines.extend([
        "",
        "## Task Results",
        "",
    ])

    for task_result in results.get("task_results", []):
        status = "✅" if task_result["passed"] else "❌"
        lines.append(f"- {status} **{task_result['task_id']}**: {task_result['attempts']} attempts, score {task_result['score']:.2f}")

    lines.extend([
        "",
        "---",
        f"*Generated by run_matrix_benchmark.py*",
    ])

    with open(filepath, "w") as f:
        f.write("\n".join(lines))

    return filepath


def load_previous_sessions(output_path: Path) -> list[dict]:
    """Load all previous session results."""
    sessions = []
    # Match both old format (session_*_raw.json) and new format (*_session_*_raw.json)
    for filepath in sorted(output_path.glob("*session_*_raw.json")):
        try:
            with open(filepath) as f:
                data = json.load(f)
                sessions.append(data)
        except (json.JSONDecodeError, IOError):
            continue
    return sessions


def generate_comparison_report(output_path: Path) -> Path:
    """Generate comparison report across all sessions."""
    sessions = load_previous_sessions(output_path)

    if len(sessions) < 2:
        print("Need at least 2 sessions for comparison.")
        return None

    filepath = output_path / "comparison.md"

    lines = [
        "# Cross-Session Comparison Report",
        "",
        f"**Sessions analyzed:** {len(sessions)}",
        f"**Generated:** {datetime.now().isoformat()}",
        "",
        "## Summary",
        "",
        "This report shows how performance changes across benchmark sessions,",
        "demonstrating the effect of episodic learning.",
        "",
        "## Session-over-Session Performance",
        "",
        "| Session | Episodes | Pass Rate | Avg Attempts | Avg Score |",
        "|---------|----------|-----------|--------------|-----------|",
    ]

    for session_data in sessions:
        session_num = session_data.get("session", "?")
        episodes = session_data.get("episodes_at_start", 0)

        # Calculate aggregates across all strategy/model combos
        matrix = session_data.get("matrix", [])
        if matrix:
            avg_pass_rate = sum(r["pass_rate"] for r in matrix) / len(matrix)
            avg_attempts = sum(r["avg_attempts"] for r in matrix) / len(matrix)
            avg_score = sum(r["avg_score"] for r in matrix) / len(matrix)
        else:
            avg_pass_rate = avg_attempts = avg_score = 0

        lines.append(
            f"| {session_num} | {episodes} | {avg_pass_rate:.1%} | {avg_attempts:.2f} | {avg_score:.2f} |"
        )

    # Calculate improvement
    if len(sessions) >= 2:
        first = sessions[0]
        last = sessions[-1]

        first_matrix = first.get("matrix", [])
        last_matrix = last.get("matrix", [])

        if first_matrix and last_matrix:
            first_pass = sum(r["pass_rate"] for r in first_matrix) / len(first_matrix)
            last_pass = sum(r["pass_rate"] for r in last_matrix) / len(last_matrix)
            improvement = last_pass - first_pass

            first_attempts = sum(r["avg_attempts"] for r in first_matrix) / len(first_matrix)
            last_attempts = sum(r["avg_attempts"] for r in last_matrix) / len(last_matrix)
            attempts_reduction = first_attempts - last_attempts

            lines.extend([
                "",
                "## Improvement Analysis",
                "",
                f"**Pass rate change:** {improvement:+.1%} (Session 1 → Session {len(sessions)})",
                f"**Attempts reduction:** {attempts_reduction:+.2f} attempts",
                "",
            ])

            if improvement > 0:
                lines.append("✅ **Learning is working!** Performance improved across sessions.")
            elif improvement < 0:
                lines.append("⚠️ Performance decreased. Consider reviewing episode quality.")
            else:
                lines.append("➡️ Performance unchanged.")

    # Strategy comparison across sessions
    lines.extend([
        "",
        "## Strategy Performance by Session",
        "",
    ])

    strategies = set()
    for s in sessions:
        for r in s.get("matrix", []):
            strategies.add(r["strategy"])

    for strategy in sorted(strategies):
        lines.append(f"### {strategy}")
        lines.append("")
        lines.append("| Session | Pass Rate | Avg Attempts |")
        lines.append("|---------|-----------|--------------|")

        for session_data in sessions:
            session_num = session_data.get("session", "?")
            strategy_results = [r for r in session_data.get("matrix", []) if r["strategy"] == strategy]
            if strategy_results:
                avg_pass = sum(r["pass_rate"] for r in strategy_results) / len(strategy_results)
                avg_att = sum(r["avg_attempts"] for r in strategy_results) / len(strategy_results)
                lines.append(f"| {session_num} | {avg_pass:.1%} | {avg_att:.2f} |")

        lines.append("")

    lines.extend([
        "---",
        "*Generated by run_matrix_benchmark.py --compare*",
    ])

    with open(filepath, "w") as f:
        f.write("\n".join(lines))

    return filepath


def save_conversation_log(output_path: Path, session: int, task_results: list[dict], timestamp: str) -> Path:
    """Save detailed conversation logs in human-readable format."""
    log_path = output_path / f"{timestamp}_session_{session}_conversations.md"

    lines = [
        f"# Conversation Log - Session {session}",
        "",
        f"Generated: {datetime.now().isoformat()}",
        "",
    ]

    for result in task_results:
        lines.extend([
            f"## Task: {result['task_id']}",
            "",
            f"**Strategy:** {result['strategy']}",
            f"**Model:** {result['model']}",
            f"**Result:** {'✅ PASS' if result['passed'] else '❌ FAIL'}",
            f"**Attempts:** {result['attempts']}",
            f"**Score:** {result['score']:.2f}",
            "",
        ])

        # Add trajectory if available
        if result.get('trajectory'):
            lines.append("### Execution Trajectory")
            lines.append("")
            for i, step in enumerate(result['trajectory'], 1):
                step_type = step.get('step', 'unknown')
                if step_type == 'prompt_built':
                    prompt = step.get('prompt', '')[:500]
                    lines.extend([
                        f"**Step {i}: Prompt Built**",
                        "```",
                        prompt + ("..." if len(step.get('prompt', '')) > 500 else ""),
                        "```",
                        "",
                    ])
                elif step_type == 'llm_response':
                    response = step.get('response', '')[:500]
                    lines.extend([
                        f"**Step {i}: LLM Response**",
                        "```",
                        response + ("..." if len(step.get('response', '')) > 500 else ""),
                        "```",
                        "",
                    ])
                elif step_type == 'code_extracted':
                    code = step.get('code', '')
                    lines.extend([
                        f"**Step {i}: Code Extracted**",
                        "```python",
                        code,
                        "```",
                        "",
                    ])
                elif step_type == 'code_executed':
                    exec_result = step.get('result', {})
                    if exec_result.get('success'):
                        output = str(exec_result.get('output', ''))[:200]
                        lines.extend([
                            f"**Step {i}: Code Executed - SUCCESS**",
                            f"Output: `{output}`",
                            "",
                        ])
                    else:
                        error = str(exec_result.get('error', ''))[:200]
                        lines.extend([
                            f"**Step {i}: Code Executed - FAILED**",
                            f"Error: `{error}`",
                            "",
                        ])

        lines.extend(["---", ""])

    with open(log_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))

    return log_path


def run_benchmark(
    session: int,
    storage_path: Path,
    output_path: Path,
    model: str,
    num_tasks: int,
    max_attempts: int,
) -> dict:
    """Run the benchmark and return results."""

    print(f"Multi-Model Benchmark - Session {session}")
    print("=" * 60)

    # Setup storage - always use FileStorage for persistence
    storage = FileStorage(storage_path)
    print(f"Using persistent storage: {storage_path}")

    # Setup strategies with shared storage
    strategies = {
        "none": NoImprovementStrategy(),
        "reflection": ReflectionStrategy(storage=storage),
        "episodic": EpisodicMemoryStrategy(storage=storage),
    }

    # Count episodes at start
    episode_count = 0
    if isinstance(storage, FileStorage):
        episode_count = len(storage.list_keys("episodes"))
    print(f"Episodes at session start: {episode_count}")

    # Setup tasks
    tasks = create_task_suite(include_easy=True, include_medium=True)[:num_tasks]
    print(f"Running {len(tasks)} tasks")

    # Setup model
    models = [
        ModelConfig(name=model, provider="anthropic", temperature=0.0),
    ]
    print(f"Using model: {model}")

    # Setup executor and evaluator
    llm_client = ClaudeClient(model=model)
    executor = LLMExecutor(llm_client=llm_client)
    evaluator = ExactMatchEvaluator(numeric_tolerance=0.01, relative_tolerance=0.01)

    print(f"Max attempts per task: {max_attempts}")
    print()

    # Create config
    config = BenchmarkConfig(
        max_attempts=max_attempts,
        strategies=strategies,
        task_suite=tasks,
        models=models,
        evaluator=evaluator,
        executor=executor,
    )

    # Run benchmark
    print("Running benchmark...")
    print("-" * 60)

    runner = MatrixBenchmarkRunner(config, storage=storage)
    result = runner.run_matrix()

    # Collect results
    matrix_data = []
    task_results = []

    for strategy_name, model_results in result.get_matrix().items():
        for model_key, res in model_results.items():
            matrix_data.append({
                "strategy": strategy_name,
                "model": res.model_config.name,
                "pass_rate": res.metrics.pass_rate,
                "avg_attempts": res.metrics.avg_attempts_to_pass,
                "avg_score": res.metrics.avg_final_score,
            })

            for task_res in res.metrics.per_task_results:
                # Extract trajectory if available
                trajectory = None
                if task_res.final_result and task_res.final_result.trajectory:
                    trajectory = task_res.final_result.trajectory

                task_results.append({
                    "strategy": strategy_name,
                    "model": res.model_config.name,
                    "task_id": task_res.task.id,
                    "passed": task_res.passed,
                    "attempts": task_res.attempts,
                    "score": task_res.final_score,
                    "trajectory": trajectory,
                })

    # Build results dict
    results = {
        "session": session,
        "timestamp": datetime.now().isoformat(),
        "episodes_at_start": episode_count,
        "num_tasks": len(tasks),
        "max_attempts": max_attempts,
        "models": [m.name for m in models],
        "strategies": list(strategies.keys()),
        "matrix": matrix_data,
        "task_results": task_results,
        "best_performers": sorted(matrix_data, key=lambda x: -x["pass_rate"])[:3],
    }

    # Count episodes at end
    if isinstance(storage, FileStorage):
        episodes_end = len(storage.list_keys("episodes"))
        results["episodes_at_end"] = episodes_end
        results["episodes_created"] = episodes_end - episode_count
        print(f"\nEpisodes created this session: {episodes_end - episode_count}")

    return results


def main():
    parser = argparse.ArgumentParser(
        description="Multi-session matrix benchmarking with persistent results"
    )
    parser.add_argument(
        "--session",
        type=int,
        default=1,
        help="Session number (for tracking and naming outputs)",
    )
    parser.add_argument(
        "--storage",
        type=str,
        default=".benchmark_memory",
        help="Path for persistent episode storage (default: .benchmark_memory)",
    )
    parser.add_argument(
        "--output",
        type=str,
        default="results",
        help="Output directory for results (default: results/)",
    )
    parser.add_argument(
        "--model",
        type=str,
        default="claude-sonnet-4-5-20250929",
        help="Model to use for code generation",
    )
    parser.add_argument(
        "--tasks",
        type=int,
        default=5,
        help="Number of tasks to run (default: 5)",
    )
    parser.add_argument(
        "--max-attempts",
        type=int,
        default=3,
        help="Max attempts per task (default: 3)",
    )
    parser.add_argument(
        "--compare",
        action="store_true",
        help="Generate comparison report from existing sessions (don't run benchmark)",
    )

    args = parser.parse_args()

    output_path = create_output_dir(Path(args.output))

    # Just generate comparison report
    if args.compare:
        print("Generating comparison report...")
        report_path = generate_comparison_report(output_path)
        if report_path:
            print(f"Comparison report saved to: {report_path}")
            print()
            # Print the report
            with open(report_path) as f:
                print(f.read())
        return

    # Check for API key
    if not os.environ.get("ANTHROPIC_API_KEY"):
        print("ERROR: ANTHROPIC_API_KEY environment variable not set.")
        print("Set it in .env file or export it in your shell.")
        return

    storage_path = Path(args.storage)

    # Run benchmark
    results = run_benchmark(
        session=args.session,
        storage_path=storage_path,
        output_path=output_path,
        model=args.model,
        num_tasks=args.tasks,
        max_attempts=args.max_attempts,
    )

    # Get episode count for markdown
    episode_count = results.get("episodes_at_start", 0)

    # Generate timestamp for all output files
    timestamp = get_timestamp_prefix()

    # Save results
    json_path = save_results_json(results, output_path, args.session, timestamp)
    md_path = save_results_markdown(results, output_path, args.session, episode_count, timestamp)

    # Save conversation log for human review
    conv_path = save_conversation_log(output_path, args.session, results.get("task_results", []), timestamp)

    print()
    print("=" * 60)
    print("Results Summary")
    print("=" * 60)

    # Print matrix
    print()
    print("Strategy x Model Performance:")
    print("-" * 60)
    print(f"{'Strategy':<12} {'Model':<25} {'Pass Rate':<10} {'Attempts':<10} {'Score':<10}")
    print("-" * 60)

    for row in results["matrix"]:
        print(
            f"{row['strategy']:<12} {row['model'][:24]:<25} {row['pass_rate']:>8.1%} "
            f"{row['avg_attempts']:>8.2f} {row['avg_score']:>8.2f}"
        )

    print()
    print(f"Results saved to:")
    print(f"  - {json_path} (raw data)")
    print(f"  - {md_path} (summary)")
    print(f"  - {conv_path} (conversation logs)")
    print(f"  - {storage_path}/ (episode memory for cross-session learning)")

    # Check if we can generate comparison
    sessions = load_previous_sessions(output_path)
    if len(sessions) >= 2:
        print()
        print("Multiple sessions detected. Generating comparison...")
        comp_path = generate_comparison_report(output_path)
        if comp_path:
            print(f"  - {comp_path} (comparison)")

    print()
    print("Next steps:")
    print(f"  Run session {args.session + 1}: uv run python scripts/run_matrix_benchmark.py --session {args.session + 1} --storage {args.storage}")
    print(f"  View comparison: uv run python scripts/run_matrix_benchmark.py --compare --output {args.output}")
    print(f"  Review conversations: cat {conv_path}")


if __name__ == "__main__":
    main()
